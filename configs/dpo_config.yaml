# ─────────────────────────────────────────────────────────────────────────────
# DPO (Direct Preference Optimization) Training Configuration
# Loss: L_DPO = -log σ( β · [log π_θ(y_w|x)/π_ref(y_w|x)
#                           - log π_θ(y_l|x)/π_ref(y_l|x)] )
# ─────────────────────────────────────────────────────────────────────────────

model:
  name: "distilgpt2"
  max_length: 256
  max_prompt_length: 128

dpo:
  beta: 0.1                   # KL penalty coefficient — higher = stay closer to reference
  loss_type: "sigmoid"        # Options: sigmoid (DPO), ipo (IPO), hinge
  label_smoothing: 0.0        # 0 = standard DPO; >0 = conservative DPO
  reference_free: false       # If true, don't use reference model (SimPO-style)

reference_model:
  name: "distilgpt2"          # Should be the SFT-fine-tuned model
  freeze: true                # Never update reference model weights

training:
  output_dir: "./outputs/dpo"
  num_train_epochs: 1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 5.0e-5
  lr_scheduler_type: "linear"
  warmup_steps: 50
  weight_decay: 0.0
  fp16: false
  logging_steps: 5
  seed: 42

data:
  dataset: "synthetic_preferences"
  format: "chosen_rejected"   # Each example has {prompt, chosen, rejected}

output:
  save_model: true
  push_to_hub: false
