{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 04: GRPO \u2014 Group Relative Policy Optimization\n\n## Learning Objectives\n- Understand GRPO as an alternative to PPO without a value function\n- Implement group sampling and advantage normalization\n- Understand the connection to DeepSeek-R1\n- Compare GRPO, PPO, and DPO"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## GRPO Algorithm\n\nFor each training step:\n1. Sample $G$ completions $\\{y_1, \\ldots, y_G\\}$ from $\\pi_{\\text{old}}$ for prompt $x$\n2. Score: $r_i = \\text{Reward}(x, y_i)$ (binary for math: 1 if correct, 0 if wrong)\n3. **Group normalize:** $\\hat{A}_i = \\frac{r_i - \\mu_r}{\\sigma_r + \\epsilon}$\n4. Update with clipped surrogate:\n\n$$\\mathcal{L}_{\\text{GRPO}} = -\\frac{1}{G}\\sum_{i=1}^G \\min\\!\\left(\\rho_i \\hat{A}_i,\\ \\text{clip}(\\rho_i, 1{-}\\epsilon, 1{+}\\epsilon)\\hat{A}_i\\right) + \\beta \\cdot \\text{KL}(\\pi_\\theta \\| \\pi_{\\text{ref}})$$\n\nwhere $\\rho_i = \\pi_\\theta(y_i|x) / \\pi_{\\text{old}}(y_i|x)$\n\n**Key insight:** Group normalization replaces the value function baseline!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# !pip install torch matplotlib numpy"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint('Ready!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Group Advantage Normalization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.training.grpo_trainer import compute_group_advantages, grpo_loss\nprint('GRPO functions imported')\n\n# Example: 8 completions, binary rewards (correct/wrong)\nrewards = torch.tensor([0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0])\nadvantages = compute_group_advantages(rewards)\n\nprint('Group rewards:    ', rewards.tolist())\nprint('Group advantages: ', [round(a, 3) for a in advantages.tolist()])\nprint(f'Mean: {advantages.mean():.6f} (should be 0)')\nprint(f'Std:  {advantages.std():.6f} (should be ~1)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Visualize Group Normalization Effect"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "np.random.seed(42)\ngroup_sizes = [2, 4, 8, 16, 32]\naccuracies = [0.2, 0.3, 0.5, 0.4, 0.6]  # Fraction of correct in each group\n\nfig, axes = plt.subplots(1, len(group_sizes), figsize=(16, 3))\nfor ax, G, acc in zip(axes, group_sizes, accuracies):\n    n_correct = int(G * acc)\n    rewards = torch.tensor([1.0]*n_correct + [0.0]*(G-n_correct))\n    advs = compute_group_advantages(rewards)\n    colors = ['green' if r == 1 else 'red' for r in rewards.tolist()]\n    ax.bar(range(G), advs.tolist(), color=colors)\n    ax.set_title(f'G={G}, acc={acc:.0%}')\n    ax.set_xlabel('Completion'); ax.axhline(0, color='black', linewidth=0.8)\nplt.suptitle('Group Advantages for Different Group Sizes', fontsize=12)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Compare GRPO vs PPO vs DPO\n\n| | PPO | DPO | GRPO |\n|--|-----|-----|------|\n| **Type** | On-policy RL | Offline preference | On-policy RL |\n| **Value fn** | Yes (critic) | No | No (group norm) |\n| **Data format** | Reward signal | Preference pairs | Reward signal |\n| **Memory** | High (2 models + critic) | Medium (2 models) | Medium (2 models) |\n| **Stability** | Moderate | High | High |\n| **Used in** | InstructGPT | Llama, Mistral | DeepSeek-R1 |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate GRPO training: rewards converging upward as model improves\nnp.random.seed(0)\nsteps = list(range(1, 81))\nmean_rewards = [0.3 + 0.5*(1-np.exp(-s/25)) + np.random.randn()*0.04 for s in steps]\n\nplt.figure(figsize=(10, 4))\nplt.plot(steps, mean_rewards, 'b-', linewidth=2, label='Mean group reward')\nplt.axhline(0.3, color='gray', linestyle='--', label='Initial accuracy')\nplt.fill_between(steps, [r-0.05 for r in mean_rewards], [r+0.05 for r in mean_rewards], alpha=0.2)\nplt.xlabel('Training Step'); plt.ylabel('Mean Reward')\nplt.title('GRPO Training: Mean Group Reward Over Time')\nplt.legend(); plt.grid(alpha=0.3)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## DeepSeek-R1 Connection\n\nDeepSeek-R1 uses GRPO with:\n- **G=8** completions per problem\n- **Binary reward:** 1 if final answer correct (verified programmatically), 0 otherwise\n- **Long chains:** Model learns to reason for many steps because only the final answer is rewarded\n- **\"Aha moment\":** Without SFT warmup, the model spontaneously learns chain-of-thought!\n\n---\n\n## Exercises\n\n1. **Group size ablation:** Try G=2, 4, 8, 16. How does advantage variance change?\n2. **Reward sparsity:** Binary vs continuous rewards. Which is more stable?\n3. **Temperature effect:** How does sampling temperature affect group diversity?\n4. **Clipping effect:** Try epsilon=0.1, 0.2, 0.5. What's the effect on training stability?\n5. **Extension:** Implement GRPO for a simple bandit problem (no LLM needed)"
  }
 ]
}