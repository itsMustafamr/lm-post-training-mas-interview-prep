{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: RLHF \u2014 Reinforcement Learning from Human Feedback\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the 3-stage RLHF pipeline\n",
    "- Implement a simple reward model from scratch\n",
    "- Implement a simplified PPO training loop\n",
    "- Understand reward hacking, KL penalty, and advantage estimation\n",
    "\n",
    "---\n",
    "\n",
    "## The RLHF Pipeline\n",
    "\n",
    "\n",
    "\n",
    "### Why the KL penalty?\n",
    "\n",
    "Without KL penalty, the policy would exploit the reward model:\n",
    "- Generate very long outputs (if reward correlates with length)\n",
    "- Generate gibberish that tricks the reward model\n",
    "- **Reward hacking** \u2014 high reward, low quality\n",
    "\n",
    "KL penalty keeps the policy close to \u03c0_SFT.\n",
    "\n",
    "### PPO Objective\n",
    "\n",
    "4215\\mathcal{L}_{\text{PPO}} = \\mathbb{E}\\left[\\min\\left(r_t A_t,\\ \text{clip}(r_t, 1{-}\\epsilon, 1{+}\\epsilon) A_t\right)\right] - \beta \\cdot \text{KL}(\\pi_\theta \\| \\pi_{\text{ref}})4215\n",
    "\n",
    "where  = \\pi_\theta(a_t|s_t) / \\pi_{\text{old}}(a_t|s_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\nsys.path.insert(0, \"..\")\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nprint(f\"PyTorch: {torch.__version__}\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build a Simple Reward Model\n",
    "\n",
    "The reward model takes text and outputs a scalar score.\n",
    "We use the **Bradley-Terry** model: (x,y) = $ scalar preference score.\n",
    "\n",
    "Training loss: $\\mathcal{L}_{\text{RM}} = -\\log \\sigma(R(y_w) - R(y_l))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.reward_model import RewardModel, bradley_terry_loss\n",
    "from transformers import AutoTokenizer, AutoModel\n\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n\n",
    "# For reward model we need hidden states \u2014 use AutoModel (no LM head)\n",
    "# In practice: fine-tune an already SFT-trained model as the backbone\n",
    "print(\"Reward model uses: transformer backbone + linear scalar head\")\n",
    "print(\"Training loss: L = -log \u03c3(R(y_chosen) - R(y_rejected))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preference Data\n",
    "\n",
    "For the reward model, we need (prompt, chosen, rejected) triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.preference_data import create_synthetic_preferences\n\n",
    "pairs = create_synthetic_preferences()\n",
    "print(f\"Preference pairs: {len(pairs)}\")\n",
    "print(\"\nExample:\")\n",
    "p = pairs[0]\n",
    "print(f\"  Chosen:   {p['chosen'][:80]}...\")\n",
    "print(f\"  Rejected: {p['rejected'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Demonstrate Advantage Estimation\n",
    "\n",
    "GAE (Generalized Advantage Estimation) computes  = \\sum_k (\\gamma\\lambda)^k \\delta_{t+k}$\n",
    "\n",
    "For text generation (single step), this simplifies to  = R_t - V(s_t)$.\n",
    "We approximate (s_t) \u0007pprox \text{mean}(R)$ for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.rlhf_trainer import compute_advantages\n\n",
    "# Simulated rewards for 8 completions\n",
    "rewards = [0.2, 0.8, 0.1, 0.9, 0.3, 0.7, 0.4, 0.6]\n",
    "advantages = compute_advantages(rewards)\n\n",
    "print(\"Rewards:    \", [round(r, 2) for r in rewards])\n",
    "print(\"Advantages: \", [round(a.item(), 3) for a in advantages])\n",
    "print(f\"Mean: {advantages.mean():.6f} (\u2248 0)\")\n",
    "print(f\"Std:  {advantages.std():.6f} (\u2248 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Reward Hacking\n",
    "\n",
    "Illustrate what happens with and without KL penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n\n",
    "# Simulate training with and without KL penalty\n",
    "steps = list(range(1, 101))\n\n",
    "# Without KL: reward increases but quality (real accuracy) plateaus/drops\n",
    "reward_no_kl  = [0.3 + 0.6*(1 - np.exp(-s/20)) + np.random.randn()*0.05 for s in steps]\n",
    "quality_no_kl = [0.4 + 0.2*(1 - np.exp(-s/20)) - 0.003*s + np.random.randn()*0.03 for s in steps]\n\n",
    "# With KL: both increase together\n",
    "reward_kl     = [0.3 + 0.4*(1 - np.exp(-s/30)) + np.random.randn()*0.05 for s in steps]\n",
    "quality_kl    = [0.4 + 0.35*(1 - np.exp(-s/30)) + np.random.randn()*0.03 for s in steps]\n\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n",
    "ax1.plot(steps, reward_no_kl, label=\"Reward model score\", color=\"red\")\n",
    "ax1.plot(steps, quality_no_kl, label=\"Real quality\", color=\"blue\", linestyle=\"--\")\n",
    "ax1.set_title(\"Without KL Penalty \u2014 Reward Hacking!\", fontsize=12)\n",
    "ax1.set_xlabel(\"Training Step\"); ax1.set_ylabel(\"Score\")\n",
    "ax1.legend(); ax1.grid(alpha=0.3)\n\n",
    "ax2.plot(steps, reward_kl, label=\"Reward model score\", color=\"red\")\n",
    "ax2.plot(steps, quality_kl, label=\"Real quality\", color=\"blue\", linestyle=\"--\")\n",
    "ax2.set_title(\"With KL Penalty \u2014 Stable Training\", fontsize=12)\n",
    "ax2.set_xlabel(\"Training Step\"); ax2.set_ylabel(\"Score\")\n",
    "ax2.legend(); ax2.grid(alpha=0.3)\n\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\n",
    "**RLHF in 3 stages:**\n",
    "1. **SFT:** Teach basic instruction following\n",
    "2. **Reward Model:** Encode human preferences as a scalar signal\n",
    "3. **PPO:** Optimize for reward while KL-regularizing vs SFT model\n\n",
    "**Key takeaways:**\n",
    "- KL penalty prevents reward hacking (crucial!)\n",
    "- Advantage estimation centers and normalizes rewards\n",
    "- PPO clipping prevents large destabilizing updates\n\n",
    "---\n\n",
    "## Exercises\n\n",
    "1. **Reward model:** Implement a neural reward model using distilgpt2 backbone\n",
    "2. **KL ablation:** Vary \u03b2 from 0 to 1 \u2014 plot the effect on generation diversity\n",
    "3. **Reward hacking demo:** Design a reward function that can be easily hacked\n",
    "4. **Advantage variance:** Compare vanilla returns vs GAE advantages\n",
    "5. **Extension:** Implement full PPO with critic network"
   ]
  }
 ]
}