{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 03: DPO \u2014 Direct Preference Optimization\n\n## Learning Objectives\n- Derive DPO loss mathematically step by step\n- Implement DPO loss from scratch in PyTorch\n- Train a model on preference pairs\n- Compare with RLHF"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Mathematical Derivation\n\nRLHF optimises:\n$$\\max_\\pi \\mathbb{E}[R(x,y)] - \\beta \\cdot \\text{KL}(\\pi \\| \\pi_{\\text{ref}})$$\n\nThe optimal policy satisfies:\n$$\\pi^*(y|x) = \\frac{\\pi_{\\text{ref}}(y|x) \\exp(R(x,y)/\\beta)}{Z(x)}$$\n\nSolving for R:\n$$R^*(x,y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)$$\n\nSubstituting into the Bradley-Terry preference model:\n$$p(y_w \\succ y_l | x) = \\sigma\\!\\left(\\beta \\log \\frac{\\pi^*(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi^*(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)$$\n\n**DPO Loss** (maximize log-likelihood of preferred responses):\n$$\\mathcal{L}_{\\text{DPO}} = -\\log \\sigma\\!\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# !pip install torch transformers tqdm matplotlib"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nprint('Ready!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Implement DPO Loss from Scratch"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.training.dpo_trainer import compute_log_probs, dpo_loss\nprint('DPO loss imported successfully')\nprint()\nprint('dpo_loss signature: (pol_chosen, pol_rejected, ref_chosen, ref_rejected, beta)')\nprint()\n# Demo with synthetic log-probs\npol_chosen   = torch.tensor([-2.0, -1.5, -1.8])\npol_rejected = torch.tensor([-3.5, -4.0, -3.2])\nref_chosen   = torch.tensor([-2.5, -2.0, -2.2])\nref_rejected = torch.tensor([-3.0, -3.5, -2.8])\nloss, acc, margin = dpo_loss(pol_chosen, pol_rejected, ref_chosen, ref_rejected, beta=0.1)\nprint(f'DPO Loss:       {loss.item():.4f}')\nprint(f'Reward Acc:     {acc.item():.4f}  (fraction where chosen > rejected)')\nprint(f'Reward Margin:  {margin.item():.4f} (how much chosen > rejected on average)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Load Preference Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.data.preference_data import create_synthetic_preferences\npairs = create_synthetic_preferences()\nprint(f'Loaded {len(pairs)} preference pairs')\nfor i, p in enumerate(pairs[:2]):\n    print(f'Pair {i+1}:')\n    print(f'  Chosen:   {p[\"chosen\"][:70]}')\n    print(f'  Rejected: {p[\"rejected\"][:70]}')\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Visualize DPO Training Dynamics\n\nKey metrics to track:\n1. **Loss** (should decrease)\n2. **Reward accuracy** (chosen reward > rejected, should approach 1.0)\n3. **Reward margin** (gap between chosen and rejected, should increase)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nnp.random.seed(42)\nsteps = list(range(1, 51))\nloss_curve   = [0.7 - 0.35*(1-np.exp(-s/15)) + np.random.randn()*0.03 for s in steps]\nacc_curve    = [0.5 + 0.45*(1-np.exp(-s/15)) + np.random.randn()*0.02 for s in steps]\nmargin_curve = [0.0 + 0.5*(1-np.exp(-s/20)) + np.random.randn()*0.02 for s in steps]\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\nfor ax, y, title, color in zip(axes,\n        [loss_curve, acc_curve, margin_curve],\n        ['DPO Loss', 'Reward Accuracy', 'Reward Margin'],\n        ['steelblue', 'green', 'darkorange']):\n    ax.plot(steps, y, color=color, linewidth=2)\n    ax.set_title(title); ax.set_xlabel('Step')\n    ax.grid(alpha=0.3)\nplt.suptitle('DPO Training Dynamics', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## DPO Variants\n\n| Variant | Change | When to use |\n|---------|--------|-------------|\n| **IPO** | Identity instead of sigmoid | Avoid overfitting to extreme pairs |\n| **KTO** | No paired data needed | Unpaired binary feedback |\n| **SimPO** | Reference-free, length-normalized | Simpler, no ref model |\n| **ORPO** | Combines SFT + DPO | More efficient, one-stage |\n\n---\n\n## Exercises\n\n1. **Vary beta:** Try beta=0.01, 0.1, 1.0. How does it affect reward margin?\n2. **IPO implementation:** Replace logsigmoid with identity loss. Compare stability.\n3. **More data:** Generate 20 preference pairs from GSM8K. Does accuracy improve?\n4. **Reference-free DPO:** What happens if you set ref_log_probs = 0?\n5. **Credit connection:** How could DPO signal be used for agent credit assignment?"
  }
 ]
}