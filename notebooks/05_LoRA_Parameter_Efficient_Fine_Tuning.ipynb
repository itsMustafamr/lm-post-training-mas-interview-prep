{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 05: LoRA & Parameter-Efficient Fine-Tuning\n\n## Learning Objectives\n- Understand LoRA's mathematical foundation\n- Implement LoRALayer from scratch\n- Compare parameter counts (full vs LoRA)\n- Demonstrate multi-agent LoRA with shared base"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LoRA Theory\n\nFull fine-tuning updates every parameter: $W \\leftarrow W_0 + \\Delta W$\n\nLoRA decomposes $\\Delta W$ as a low-rank product:\n$$\\Delta W = B \\cdot A \\quad \\text{where } B \\in \\mathbb{R}^{d \\times r}, A \\in \\mathbb{R}^{r \\times k},\\ r \\ll \\min(d, k)$$\n\nForward pass:\n$$h = W_0 x + \\frac{\\alpha}{r} BAx$$\n\n**Initialization:** $A \\sim \\mathcal{N}(0, \\sigma^2)$, $B = 0$ (so $\\Delta W = 0$ initially)\n\n**Parameter reduction:**\n- Full fine-tuning: $d \\times k$ params\n- LoRA (rank r): $r(d + k)$ params\n- Example: $d=k=768$, $r=8$: $589{,}824 \\to 12{,}288$ (**98% reduction**)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# !pip install torch matplotlib"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nprint('Ready!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Implement LoRA from Scratch"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.training.lora_utils import LoRALayer, apply_lora_to_linear, count_parameters, freeze_base_model\n\n# Create a LoRA layer\nlora = LoRALayer(in_features=768, out_features=768, r=8, alpha=16.0)\nprint(f'LoRA layer created')\nprint(f'  in=768, out=768, r=8, alpha=16')\nprint(f'  Scaling factor: alpha/r = {lora.scaling:.2f}')\nprint(f'  A shape: {lora.lora_A.weight.shape}')\nprint(f'  B shape: {lora.lora_B.weight.shape}')\nprint(f'  Trainable params: {lora.trainable_parameters():,}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare parameter counts\nd, k = 768, 768\nprint('Parameter count comparison:')\nprint(f'{\"Rank\":<8} {\"LoRA Params\":<15} {\"Full Params\":<15} {\"Reduction\":<12}')\nprint('-' * 50)\nfor r in [1, 2, 4, 8, 16, 32, 64]:\n    lora_p = r * (d + k)\n    full_p = d * k\n    reduction = (1 - lora_p/full_p) * 100\n    print(f'{r:<8} {lora_p:<15,} {full_p:<15,} {reduction:.1f}%')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Multi-Agent LoRA Architecture\n\nThe key insight: one frozen base model, multiple lightweight adapters.\n\n```\nGPU Memory:\n  [Frozen Base Model \u2014 1.5 GB for distilgpt2]\n  + [Solver Adapter A_s, B_s \u2014 0.05 MB for r=8]\n  + [Critic Adapter A_c, B_c \u2014 0.05 MB for r=8]\n  + [Reviser Adapter A_r, B_r \u2014 0.05 MB for r=8]\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate: apply LoRA to a linear layer and verify forward pass\nlinear = nn.Linear(768, 768)\nfreeze_base_model(linear)\nlora_layer = apply_lora_to_linear(linear, r=8, alpha=16.0)\n\nx = torch.randn(4, 32, 768)  # batch=4, seq=32, hidden=768\nwith torch.no_grad():\n    base_out = linear(x)\n    lora_out = lora_layer(x)\n\nprint(f'Input shape:  {x.shape}')\nprint(f'Base output:  {base_out.shape}')\nprint(f'LoRA output:  {lora_out.shape}')\ndelta = (lora_out - base_out).abs().mean()\nprint(f'Delta W effect (should be small initially): {delta:.6f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n1. **Rank sensitivity:** Try r=1, 4, 8, 64. How does generation quality vs param count trade off?\n2. **Alpha tuning:** With r=8, try alpha=8 (scaling=1), 16 (scaling=2), 32 (scaling=4)\n3. **Multi-agent demo:** Create solver/critic/reviser LoRA adapters with different random inits\n4. **Weight merging:** Implement merge_lora_weights() and verify W_merged = W0 + scaling*B*A\n5. **QLoRA:** Research QLoRA (quantized LoRA) \u2014 how does 4-bit quantization + LoRA work?"
  }
 ]
}