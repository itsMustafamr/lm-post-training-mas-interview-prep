{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: Supervised Fine-Tuning (SFT)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand *why* SFT is needed and what it does\n",
    "- Implement SFT training loop from scratch (manual PyTorch)\n",
    "- Learn about label masking (why we mask instruction tokens)\n",
    "- Compare before/after SFT generation quality\n",
    "\n",
    "---\n",
    "\n",
    "## What is SFT?\n",
    "\n",
    "A pre-trained language model (GPT-2, LLaMA, etc.) is trained to **predict the next token** on a massive text corpus. It learns *how language works*, but it doesn't know how to *answer questions* or *follow instructions*.\n",
    "\n",
    "**SFT (Supervised Fine-Tuning)** teaches the model to follow instructions by showing it examples of (instruction, good response) pairs.\n",
    "\n",
    "### The Loss Function\n",
    "\n",
    "$$\\mathcal{L}_{\\text{SFT}} = -\\frac{1}{T} \\sum_{t=1}^{T} \\log p_\\theta(y_t \\mid y_{<t}, x)$$\n",
    "\n",
    "where:\n",
    "- $x$ = the instruction (input prompt)\n",
    "- $y_t$ = the $t$-th response token\n",
    "- $T$ = total number of **response** tokens (NOT instruction tokens)\n",
    "\n",
    "### Label Masking\n",
    "\n",
    "```\n",
    "Input:  [Instruction tokens...] [Response tokens...]\n",
    "Labels: [   -100   -100 ...  ] [  actual token IDs ]\n",
    "          ← masked (ignored) →  ← compute loss here →\n",
    "```\n",
    "\n",
    "We mask instruction tokens with `-100` (PyTorch's ignore index) so the loss is only computed on the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run this cell first on Google Colab)\n",
    "# !pip install torch transformers tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a Synthetic Math Instruction Dataset\n",
    "\n",
    "For demos we create a small synthetic dataset. In real training you'd use datasets like:\n",
    "- OpenOrca, Alpaca, ShareGPT (instruction following)\n",
    "- MetaMath, MathInstruct (math-specific SFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic math instruction dataset\n",
    "MATH_INSTRUCTIONS = [\n",
    "    {\"instruction\": \"A store has 45 apples. It sells 18. How many remain?\",\n",
    "     \"response\": \"Step 1: Start with 45 apples.\\nStep 2: Subtract 18 sold: 45 - 18 = 27.\\nThe answer is: 27\"},\n",
    "    {\"instruction\": \"A train goes 60 km/h for 3 hours. Distance?\",\n",
    "     \"response\": \"Distance = speed × time = 60 × 3 = 180 km.\\nThe answer is: 180\"},\n",
    "    {\"instruction\": \"6 boxes with 12 apples each. Total apples?\",\n",
    "     \"response\": \"Total = 6 × 12 = 72 apples.\\nThe answer is: 72\"},\n",
    "    {\"instruction\": \"Rectangle 8m wide and 5m tall. Area?\",\n",
    "     \"response\": \"Area = width × height = 8 × 5 = 40 m².\\nThe answer is: 40\"},\n",
    "    {\"instruction\": \"Sarah earns $15/hour and works 8 hours. Earnings?\",\n",
    "     \"response\": \"Earnings = 15 × 8 = $120.\\nThe answer is: 120\"},\n",
    "]\n",
    "\n",
    "print(f'Dataset size: {len(MATH_INSTRUCTIONS)} examples')\n",
    "print('\\nExample:')\n",
    "ex = MATH_INSTRUCTIONS[0]\n",
    "print(f'  Instruction: {ex[\"instruction\"]}')\n",
    "print(f'  Response:    {ex[\"response\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Model and Tokenizer\n",
    "\n",
    "We use `distilgpt2` — a small (~82M param) model that runs on free Colab T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilgpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 has no pad token by default\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model: {MODEL_NAME} ({n_params/1e6:.1f}M parameters)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Pre-SFT Generation\n",
    "\n",
    "Before fine-tuning, the base model has no idea how to solve math problems in our format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=80):\n",
    "    \"\"\"Simple generation function.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    gen_ids = output[0][input_ids.shape[1]:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "test_prompt = '### Instruction:\\nA store has 45 apples. It sells 18. How many remain?\\n\\n### Response:\\n'\n",
    "print('=== PRE-SFT generation ===')\n",
    "print(f'Prompt: {test_prompt}')\n",
    "print(f'Output: {generate(model, tokenizer, test_prompt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement the SFT Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.training.sft_trainer import SFTConfig, SFTTrainer\n",
    "\n",
    "config = SFTConfig(\n",
    "    learning_rate=2e-4,\n",
    "    num_epochs=3,\n",
    "    batch_size=2,\n",
    "    max_length=128,\n",
    "    logging_steps=5,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(model, tokenizer, config)\n",
    "print('SFTTrainer ready!')\n",
    "print(f'Config: {config}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "losses = trainer.train(MATH_INSTRUCTIONS)\n",
    "print(f'Training complete! Final loss: {losses[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.visualization import plot_training_curves\n",
    "\n",
    "fig = plot_training_curves(losses, title='SFT Training Loss on Synthetic Math Dataset')\n",
    "plt.show()\n",
    "print(f'Starting loss: {losses[0]:.4f} → Final loss: {losses[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare Before vs After SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== POST-SFT generation ===')\n",
    "print(f'Prompt: {test_prompt}')\n",
    "print(f'Output: {generate(model, tokenizer, test_prompt)}')\n",
    "\n",
    "print('\\n=== Another test ===')\n",
    "prompt2 = '### Instruction:\\n6 boxes with 12 apples each. How many total?\\n\\n### Response:\\n'\n",
    "print(f'Output: {generate(model, tokenizer, prompt2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we learned:**\n",
    "1. SFT fine-tunes a pre-trained LM on instruction-response pairs\n",
    "2. Loss is computed only on response tokens (label masking with -100)\n",
    "3. Training loss decreasing shows the model is learning the format\n",
    "4. SFT is the **first stage** in RLHF and DPO pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Add more data:** Expand `MATH_INSTRUCTIONS` with 10 more problems. Does accuracy improve?\n",
    "2. **Change the instruction template:** Use a different format (e.g., `Q:` / `A:`). What changes?\n",
    "3. **Ablate masking:** What happens if you compute loss on instruction tokens too? (Remove masking)\n",
    "4. **Learning rate sweep:** Try lr=1e-5, 1e-4, 1e-3. Which gives the best results?\n",
    "5. **Extension:** Modify the trainer to support gradient checkpointing for longer sequences."
   ]
  }
 ]
}
