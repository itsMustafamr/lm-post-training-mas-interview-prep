{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 06: Reward Modeling\n\n## Learning Objectives\n- Build a reward model from scratch using Bradley-Terry\n- Understand PRM vs ORM\n- Train on preference data\n- Evaluate reward model quality"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Theory\n\nA reward model $R_\\phi(x, y) \\in \\mathbb{R}$ scores response quality.\n\n**Bradley-Terry loss:**\n$$\\mathcal{L}_{\\text{RM}} = -\\log \\sigma(R_\\phi(x, y_w) - R_\\phi(x, y_l))$$\n\nArchitecture: transformer backbone + linear scalar head.\n\n**PRM vs ORM:**\n- ORM: single reward at end of sequence\n- PRM: reward per reasoning step (Lightman et al., 2023)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# !pip install torch transformers tqdm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nprint('Ready!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Understand the Bradley-Terry Loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.training.reward_model import bradley_terry_loss\n\n# Synthetic demo\nreward_chosen   = torch.tensor([2.0, 1.5, 2.5, 1.0])\nreward_rejected = torch.tensor([0.5, 0.3, 1.0, 0.8])\nloss = bradley_terry_loss(reward_chosen, reward_rejected)\nprint(f'Reward chosen:   {reward_chosen.tolist()}')\nprint(f'Reward rejected: {reward_rejected.tolist()}')\nprint(f'BT Loss: {loss.item():.4f}')\nprint(f'Accuracy (chosen > rejected): {(reward_chosen > reward_rejected).float().mean():.2%}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Process Reward Model (Step-Level Signals)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.credit_assignment.process_reward import StepRewardModel, assign_step_rewards\n\nsolution = ('Step 1: The store starts with 45 apples.\\n'\n            'Step 2: It sells 18 apples. So 45 - 18 = 27.\\n'\n            'Step 3: The remaining count is 27.\\nThe answer is: 27')\n\nprm = StepRewardModel(use_heuristic=True)\nscored = prm.score_solution(solution, ground_truth=27.0)\nprint('PRM Step-Level Scores:')\nfor step, reward in scored:\n    print(f'  [{reward:.2f}] {step[:60]}')\n\nprint('\\nInterpolated rewards (outcome=1.0, decay=0.9):')\nfor step, reward in assign_step_rewards(solution, outcome_reward=1.0):\n    print(f'  [{reward:.3f}] {step[:60]}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Visualize PRM vs ORM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# ORM: single signal at end\nsteps = [1, 2, 3, 4, 5]\norm_signal = [0, 0, 0, 0, 1]\nprs_signal  = [0.59, 0.66, 0.73, 0.81, 1.0]\nax1.bar(steps, orm_signal, color='steelblue', alpha=0.7, label='ORM')\nax1.set_title('ORM: Single Terminal Reward')\nax1.set_xlabel('Step'); ax1.set_ylabel('Reward'); ax1.legend()\nax2.bar(steps, prs_signal, color='green', alpha=0.7, label='PRM')\nax2.set_title('PRM: Per-Step Reward')\nax2.set_xlabel('Step'); ax2.set_ylabel('Reward'); ax2.legend()\nplt.suptitle('ORM vs PRM Signal Density', fontsize=13)\nplt.tight_layout(); plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Exercises\n\n1. **Bradley-Terry demo:** Show that BT loss approaches 0 as reward gap grows\n2. **PRM interpolation:** Try decay=0.5, 0.8, 0.9, 1.0. How does it affect step signals?\n3. **Wrong solution:** Score a wrong solution with PRM. Where does it first diverge?\n4. **Extension:** Implement a neural PRM backbone using distilgpt2\n5. **Calibration:** How do you know if a reward model is well-calibrated?"
  }
 ]
}